{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18999d02-35b7-40d6-bdc6-baa90477ed29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/quinten/Utilisateurs/slemouellic/.local/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "No model was supplied, defaulted to google/vit-base-patch16-224 and revision 5dca96d (https://huggingface.co/google/vit-base-patch16-224).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarization_model = pipeline(\"summarization\")\n",
    "sentiment_model = pipeline(\"sentiment-analysis\")\n",
    "translation_model = pipeline('translation_en_to_fr')\n",
    "image_classification = pipeline(\"image-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b93490-5aed-4beb-a35e-8e451e5f81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import download_file\n",
    "\n",
    "def new_predict(sentiment_model,summarization_model, translation_model,image_classification,input_type, data = None,min_length = 0, max_length = 150):\n",
    "    \"\"\"\n",
    "    make and combine the prediction of all the differents models on the differents scoring tables\n",
    "\n",
    "    params_scoring dictionnary: dictionnary that contain all the scoring tables on which models will...\n",
    "    ... make predictions. It have to be a dictionnary as the predict function of mlflow only take a single argument.\n",
    "\n",
    "    Return Pandas dataframe with all the fraud risk score (and few others informations) on the remise batch\n",
    "    Return interpretation_remises pandas dataframe with the interpretation for the remises model\n",
    "    Return interpretation_client pandas dataframe with the interpretation for the clients models\n",
    "    \"\"\"\n",
    "    \n",
    "    if input_type == \"sentiment\":\n",
    "        return sentiment_model(data)\n",
    "\n",
    "    elif input_type == \"summarization\":\n",
    "\n",
    "        if data is None:\n",
    "            data = download_file.download_story()\n",
    "\n",
    "        dict_result = summarization_model(data, min_length, max_length)[0]\n",
    "        dict_result[\"input_text\"] = data\n",
    "        return dict_result\n",
    "\n",
    "    elif input_type == \"translation\":\n",
    "        return translation_model(data)\n",
    "\n",
    "    elif input_type == \"image\":\n",
    "        image = download_file.download_image(data)\n",
    "        return image_classification(image)\n",
    "\n",
    "    return \"mauvais type selectionné\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "680994ac-da52-4de6-94ac-646d62f044d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pkgutil\n",
    "import importlib\n",
    "import sys\n",
    "import cloudpickle\n",
    "import mlflow\n",
    "\n",
    "\n",
    "\n",
    "def variable_for_value(value):\n",
    "    for n,v in globals().items():\n",
    "        if v == value:\n",
    "            return n\n",
    "    return None\n",
    "\n",
    "class custom_ml_flow_model(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "    def load_context(self, context):\n",
    "        \n",
    "        import cloudpickle\n",
    "        import inspect\n",
    "        \n",
    "        list_artifacts_name = []\n",
    "        \n",
    "        for name, artifact in context.artifacts.items():\n",
    "            list_artifacts_name.append(name)\n",
    "            print(name)\n",
    "            with open(artifact, \"rb\") as f:\n",
    "                exec(\"self.{} =  cloudpickle.load(f)\".format(name))\n",
    "            print(\"chargé\")\n",
    "                \n",
    "\n",
    "        signature = inspect.signature(self.__predict__)\n",
    "\n",
    "        self.arg_default_value = {\n",
    "                                k: v.default\n",
    "                                for k, v in signature.parameters.items()\n",
    "                                if v.default is not inspect.Parameter.empty\n",
    "                                }\n",
    "\n",
    "        list_args_predict = [k for k, v in signature.parameters.items()]\n",
    "\n",
    "        dict_args___predict__ = {}\n",
    "        \n",
    "        for arg in list_args_predict:\n",
    "            if arg in list_artifacts_name:\n",
    "                dict_args___predict__[arg] = \"self.{}\".format(arg)\n",
    "            else :\n",
    "                dict_args___predict__[arg] = \"params_predict[\\\"{}\\\"]\".format(arg)\n",
    "                \n",
    "        self.dict_args___predict__ = dict_args___predict__\n",
    "        \n",
    "    def register_all_function_by_value(self):    \n",
    "        \n",
    "        sub_folder_names = [(x) for x in os.listdir() if os.path.isdir(x)]\n",
    "    \n",
    "        for fold_name, folder in [(None, os.getcwd())] + [(x, os.path.join(os.getcwd(), x)) for x in os.listdir() if os.path.isdir(x)]:\n",
    "            if \".ipynb_\" not in folder:\n",
    "                for module in [name for _, name, _ in pkgutil.iter_modules([folder]) if name not in sub_folder_names]:\n",
    "                    print(module)\n",
    "                    full_module = module if fold_name is None else fold_name + \".\" + module\n",
    "                    module_spec = importlib.util.spec_from_file_location(full_module,os.path.join(folder, \"{}.py\".format(module)))\n",
    "                    module = importlib.util.module_from_spec(module_spec)\n",
    "                    sys.modules[module_spec.name] = module\n",
    "                    module_spec.loader.exec_module(module)\n",
    "                    cloudpickle.register_pickle_by_value(module)\n",
    "            \n",
    "    def pickle_artifacts(self, artifact_path, predict_function, models):\n",
    "        artifacts = {}\n",
    "        \n",
    "        name_pred = \"__predict__\"\n",
    "        path_pred = os.path.join(artifact_path, '{}.pkl'.format(name_pred))\n",
    "        with open(path_pred, \"wb\") as f:\n",
    "            cloudpickle.dump(predict_function, f)\n",
    "            \n",
    "            artifacts[name_pred] = path_pred\n",
    "\n",
    "        for model in models:\n",
    "            name = variable_for_value(model)\n",
    "            path = os.path.join(artifact_path, \"{}.pkl\".format(name))\n",
    "            with open(path, \"wb\") as f:\n",
    "                cloudpickle.dump(model, f)\n",
    "                \n",
    "            artifacts[name] = path\n",
    "            \n",
    "        return artifacts\n",
    "        \n",
    "                \n",
    "    def save(self, artifact_path, predict_function, models):\n",
    "        \n",
    "        self.register_all_function_by_value()\n",
    "        \n",
    "        artifacts = self.pickle_artifacts(artifact_path, predict_function, models)\n",
    "                \n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path = \"Model\",\n",
    "            python_model   =  self,\n",
    "            artifacts      =  artifacts)\n",
    "        \n",
    "        print(artifacts)\n",
    "        \n",
    "        for name, inter_model in artifacts.items():\n",
    "            os.remove(artifacts[name])\n",
    "                            \n",
    "    def predict(self, context, params_predict): \n",
    "        \"\"\"\n",
    "        make and combine the prediction of all the differents models on the differents scoring tables\n",
    "    \n",
    "        params_scoring dictionnary: dictionnary that contain all the scoring tables on which models will...\n",
    "        ... make predictions. It have to be a dictionnary as the predict function of mlflow only take a single argument.\n",
    "    \n",
    "        Return Pandas dataframe with all the fraud risk score (and few others informations) on the remise batch\n",
    "        Return interpretation_remises pandas dataframe with the interpretation for the remises model\n",
    "        Return interpretation_client pandas dataframe with the interpretation for the clients models\n",
    "        \"\"\"\n",
    "        \n",
    "        for arg in self.arg_default_value:\n",
    "            if arg not in params_predict.keys():\n",
    "                params_predict[arg] = self.arg_default_value[arg]\n",
    "        \n",
    "        new_dict = {}\n",
    "        \n",
    "        for x, y in self.dict_args___predict__.items():\n",
    "            new_dict[x] = eval(y)  \n",
    "            \n",
    "        return self.__predict__(**new_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8cbe3a5-36cf-49ec-85d1-c0b2170f38b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/quinten/Utilisateurs/slemouellic/New_test_ml_flow/test_artifacts/mlruns/quinten/84ea04c00aee411bb3905607782786ad\n",
      "download_file\n",
      "no_import\n",
      "to_import\n",
      "utils\n",
      "test_import\n",
      "{'__predict__': '/home/quinten/Utilisateurs/slemouellic/New_test_ml_flow/test_artifacts/mlruns/quinten/84ea04c00aee411bb3905607782786ad/__predict__.pkl', 'summarization_model': '/home/quinten/Utilisateurs/slemouellic/New_test_ml_flow/test_artifacts/mlruns/quinten/84ea04c00aee411bb3905607782786ad/summarization_model.pkl', 'image_classification': '/home/quinten/Utilisateurs/slemouellic/New_test_ml_flow/test_artifacts/mlruns/quinten/84ea04c00aee411bb3905607782786ad/image_classification.pkl', 'translation_model': '/home/quinten/Utilisateurs/slemouellic/New_test_ml_flow/test_artifacts/mlruns/quinten/84ea04c00aee411bb3905607782786ad/translation_model.pkl', 'sentiment_model': '/home/quinten/Utilisateurs/slemouellic/New_test_ml_flow/test_artifacts/mlruns/quinten/84ea04c00aee411bb3905607782786ad/sentiment_model.pkl'}\n"
     ]
    }
   ],
   "source": [
    "tracking_uri =\"/home/quinten/Utilisateurs/slemouellic/New_test_ml_flow/test_artifacts/mlruns/\"\n",
    "experiment_id = \"quinten\" \n",
    "\n",
    "mlflow.set_tracking_uri(tracking_uri)        \n",
    "mlflow_experiment_path = os.path.join(tracking_uri,experiment_id)\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment_id) as run:\n",
    "        mlruns_artifcats_path = os.path.join(mlflow_experiment_path, run.info.run_id)\n",
    "        print(mlruns_artifcats_path)\n",
    "        \n",
    "        model = custom_ml_flow_model()\n",
    "        \n",
    "        model.save(artifact_path = mlruns_artifcats_path,\n",
    "                    predict_function = new_predict, \n",
    "                   models = [summarization_model, \n",
    "                             image_classification,\n",
    "                            translation_model,\n",
    "                            sentiment_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ce43e-26b3-4b01-96ca-2b30d04d0d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
